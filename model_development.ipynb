{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6g846ekl7Ag2HumUZVGnq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nniikkoollaass/forecasting-customer-outflow/blob/model-development/model_development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Вирішив зосередитися на логістичній регресії тому, що це ефективний метод для бінарної класифікації -\n",
        " прогнозування відтоку клієнтів. Також вибрив підхід аналізу підходу для цієї задачі,\n",
        " бо інша людина займається аналізом та підготовкою даних, і я маю час для певної підготовки коду та\n",
        "  й для вибору підходу для побудови моделі. Використоавши цей час, та зробивши нижче наведені висновки,\n",
        "   я тому і вибрав для такої задачі і такого Датасету саме логістичну регресію. Та також враховуючи, обмеження в часі, після аналізу підходів, я зупиняюся на цьому.\n",
        "\n",
        "Тобто модель повинна класифікувати кожного клієнта в одну з двох груп: клієнт, який припинить користуватися послугами компанії\n",
        " (покине нас - Клас 1 (позитивний клас)), або клієнт, який продовжить користуватися послугами компанії (залишиться з нами - Клас 0 (негативний клас)).\n",
        "\n",
        "Детальніше про розробку моделі:\n",
        "- модель розробляємо, щоб виявити ймовірність відтоку клієнтів на основі зібраних реальних даних\n",
        "- реалізовувати логіку будемо через підхід логістичної регресії\n",
        "- аналізуватися будуть всі дані про клієнтів, які наведені в Датасеті\n",
        "- використовуватимемо мітку - значення (1 або 0), тобто факт відтоку (клієнт може нас покинути) або його відсутність (клієнт певно лишиться з нами).\n",
        "\n",
        "Я вибрав саме підхід логістичної регресії, бо цей підхід надає ймовірність приналежності до класу для кожної характеристики клієнта\n",
        " (надає розуміння впливу будь-якої ознаки на ймовірність відтоку), його досить легко прописати і коригувати (тобто реалізувати),\n",
        " добре працює з різними наборами даних та може надати хорошу точність якщо дані правильно підготовлені (зазвичай надає високу точність прогнозування на багатьох наборах даних).\n",
        "\n",
        "Тобто логістична регресія полягає в її здатності надавати легко інтерпретовані ймовірнісні оцінки,\n",
        "що робить її особливо корисною для задач бінарної класифікації,\n",
        "таких як прогнозування відтоку клієнтів.\n",
        "Це дозволяє аналітикам не тільки передбачати результат, але й розуміти ступінь впевненості у цих прогнозах, що є критично важливим для прийняття бізнес-рішень.\n",
        "\n",
        "Під час навчання моделі буду використовувати крос-валідацію для налаштування параметрів моделі, для наступного:\n",
        "- уникнення перенавчання (це коли модель занадто добре підлаштовується під навчальні дані і не може обробляти нові з очікуваними результатами)\n",
        "- оптимізації гіперпараметрів (Для моделі може існувати багато гіперпараметрів, які потрібно налаштовувати.\n",
        "  Крос-валідація дозволяє ефективно тестувати різні комбінації гіперпараметрів на декількох підмножинах даних і вибирати оптимальні значення, які забезпечують найкращу продуктивність моделі)\n",
        "- забезпечення більної стабільності моделі (Крос-валідація використовує кілька підмножин даних для навчання і тестування.\n",
        "Це дозволяє моделі краще узагальнюватися і бути менш залежною від конкретного поділу даних.)\n",
        "- підтримання балансу між складністю мобелі та її здатністю оптимально опрацьовувати нові набори даних\n",
        "- оцінювання моделі через призму різних метрик\n",
        "Тому крос-валідацію краще застосовувати при налаштуванні параметрів моделі для забезпечення більшої точності,\n",
        "стабільності та узагальненості (так би мовити \"здатності очікувано давати прогноз на реальних, не навчальних даних\") моделі,\n",
        "яка може ефективно працювати на нових даних і допомагає уникнути перенавчання.\n",
        "\n",
        "Наступні підходи не викорисуються, бо\n",
        "- 1 - Лінійна регресія - краще підходить для задач передбачення кількісних значень, тоді як завдання прогнозування відтоку клієнтів є бінарною класифікацією\n",
        "може передбачити значення, що лежать поза межами [0, 1], що не має сенсу для ймовірностей\n",
        "- 2 - Метод опорних векторів (SVM) - є обчислювально витратними, особливо для великих наборів даних\n",
        "та не надає безпосередньої інтерпретації у вигляді ймовірностей, що може бути корисним для бізнес-аналітики\n",
        "- 3 - Дерева рішень - схильні до перенавчання; Хоча дерева рішень легко інтерпретуються, вони можуть бути нестабільними,\n",
        "оскільки навіть невеликі зміни у даних можуть призвести до значних змін у структурі дерева\n",
        "- 4 - Кластеризація (K-means) - підходить для задач сегментації або кластеризації, де потрібно розбити дані на групи, але не для бінарної класифікації\n",
        "- 5 - Нейронні мережі - Навчання нейронних мереж вимагає значних обчислювальних ресурсів і часу, що може бути надмірним для задачі з невеликою кількістю ознак\n",
        "\n",
        "При оціюванні точноті моделі планую використати метрики точності, повноти, точності передбачення та F1-міри."
      ],
      "metadata": {
        "id": "oj8vAAobZ7Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "# завантаження даних - потім переробиться на підготовлені дані\n",
        "data = pd.read_csv('/mnt/data/internet_service_churn.csv')\n",
        "\n",
        "# Розділяємо дані на ознаки (X) та мітки (y)\n",
        "# видаляємо колонку 'сhurn' з набору даних data, залишаючи всі інші колонки як ознаки. axis=1 означає, що ми видаляємо колонку, а не рядок\n",
        "x = data.drop('churn', axis=1)  # будуть ознаки - є даними, на основі яких ми будемо робимо прогнози\n",
        "# вибираємо колонку 'сhurn' як цільову змінну, яку модель буде прогнозувати\n",
        "y = data['churn'] # це мітка - є тим, що ми намагаємося передбачити\n",
        "\n",
        "# розподіл даних на навчальну та тестову вибірки\n",
        "# dикористовуємо train_test_split для розподілу даних на навчальну та тестову вибірки\n",
        "# test_size: відсоток даних, що будуть використані для тестування\n",
        "# random_state є параметром, який використовується для фіксації початкового значення генератора випадкових чисел\n",
        "# фіксоване значення random_state дозволяє будь-кому отримувати однакові результати при кожному запуску коду\n",
        "x_trained, x_tested, y_trained, y_tested = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# ініціалізація моделі логістичної регресії\n",
        "# max_iter=1000 - аргумент, який задає максимальну кількість ітерацій алгоритму оптимізації\n",
        "# параметр max_iter гарантує, що алгоритм не зациклиться і завершиться після певної кількості кроків навіть, якщо не досягне оптимального розв'язку\n",
        "our_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# параметри для крос-валідації\n",
        "# визначаємо набір гіперпараметрів, які будуть використовуватися для налаштування моделі\n",
        "# 'C' - це параметр регуляризації у логістичній регресії. Він контролює ступінь регуляризації (тобто, обмеження складності моделі)\n",
        "# дозволяє знайти оптимальний баланс між підгонкою та узагальненням\n",
        "# значення 0.01, 0.1: більш сильна регуляризація (запобігає перенавчанню, але може недонавчити)\n",
        "# роблять регуляризацію сильнішою, зменшуючи ризик перенавчання, але можуть призвести до недонавчання\n",
        "# значення 1: стандартне значення, часто використовується як відправна точка\n",
        "# значення 10, 100: менш сильна регуляризація (може краще підійти для складніших моделей)\n",
        "# послаблюють регуляризацію, що може допомогти моделі краще адаптуватися до тренувальних даних, але підвищує ризик перенавчання\n",
        "# 'solver' - це алгоритм, який використовується для оптимізації функції втрат у логістичній регресії\n",
        "# значення lbfgs': Limited-memory Broyden–Fletcher–Goldfarb–Shanno (ефективний для невеликих і середніх за розміром наборів даних)\n",
        "# значення liblinear': Л=лінійний солвер, підходить для великих наборів даних\n",
        "# всі ці параметри важливі для забезпечення оптимальної продуктивності моделі, запобігання перенавчанню та адаптації до структури даних.\n",
        "parameters = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['lbfgs', 'liblinear']\n",
        "}\n",
        "\n",
        "# крос-валідація\n",
        "# використовуємо GridSearchCV для пошуку кращих параметрів\n",
        "# налаштовуємо пошук найкращих гіперпараметрів для моделі логістичної регресії за допомогою методу крос-валідації\n",
        "# 'GridSearchCV' - це клас у бібліотеці scikit-learn, який використовується для автоматичного підбору найкращих гіперпараметрів моделі\n",
        "# він перебирає всі можливі комбінації гіперпараметрів і оцінює продуктивність кожної комбінації за допомогою крос-валідації\n",
        "# вибирає ту комбінацію гіперпараметрів, яка показала найкращу середню точність на крос-валідації\n",
        "# 'estimator' вказує на модель, для якої ми хочемо підібрати гіперпараметри\n",
        "# 'param_grid' задає сітку гіперпараметрів, які потрібно перебирати\n",
        "# це словник, де ключі - це імена параметрів, а значення - списки можливих значень цих параметрів\n",
        "# 'cv' означає кількість фолдів у крос-валідації\n",
        "# ми використовуємо 5-кратну крос-валідацію, тобто дані розбиваються на 5 частин,\n",
        "# і кожна частина по черзі використовується як тестовий набір, тоді як інші частини використовуються як тренувальні набори\n",
        "# 'scoring' задає метрику, яку використовують для оцінки продуктивності моделі\n",
        "# таким чином ми забезпечуємо автоматичний пошук оптимальних гіперпараметрів,\n",
        "# запобігаємо перенавчанню та також використання крос-валідації з різними фолдами\n",
        "# дає більш об'єктивну оцінку продуктивності моделі порівняно з простою тренувальною та тестовою вибіркою\n",
        "search = GridSearchCV(estimator=our_model, param_grid=parameters, cv=5, scoring='accuracy')\n",
        "\n",
        "# навчаємо модель з кращими параметрами на повному навчальному наборі даних\n",
        "# метод fit тренує об'єкт GridSearchCV на заданих навчальних даних\n",
        "search.fit(x_trained, y_trained)\n",
        "\n",
        "# отримуємо найкращі гіперпараметри, знайдені у процесі пошуку за допомогою GridSearchCV\n",
        "# '.best_params_' - це атрибут об'єкта GridSearchCV, який зберігає найкращі гіперпараметри, знайдені під час пошуку\n",
        "# та містить словник з найкращими значеннями гіперпараметрів\n",
        "# це нам дозволяє:\n",
        "# - оптимізувати продуктивність - використання найкращих гіперпараметрів дозволяє досягти максимальної точності моделі на основі навчальних даних\n",
        "# - автоматизувати процес - GridSearchCV автоматично перебирає всі можливі комбінації гіперпараметрів, що значно спрощує роботу\n",
        "# - отримати надійність оцінки - використання крос-валідації забезпечує надійність результатів, оскільки модель оцінюється на різних підмножинах даних\n",
        "best_parameters = search.best_params_\n",
        "# друкуємо найкращі параметри\n",
        "print(f'Best parameters: {best_parameters}')\n",
        "\n",
        "# навчання моделі з кращими параметрами\n",
        "best_model = search.best_estimator_\n",
        "best_model.fit(x_trained, y_trained)\n",
        "\n",
        "# робимо передбачення на тестовій вибірці\n",
        "y_predicted = best_model.predict(x_tested)\n",
        "\n",
        "# оцінка моделі\n",
        "accuracy = accuracy_score(y_tested, y_predicted)\n",
        "recall = recall_score(y_tested, y_predicted)\n",
        "precision = precision_score(y_tested, y_predicted)\n",
        "f1 = f1_score(y_tested, y_predicted)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "id": "qKHUm3c4lpyc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsW3ylR7dlGvvCJpT3rhr5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nniikkoollaass/forecasting-customer-outflow/blob/model-development/model_development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Пояснення щодо вибору підходу для побудови моделі\n",
        "Вирішив зосередитися на логістичній регресії тому, що це ефективний метод для бінарної класифікації -\n",
        "прогнозування відтоку клієнтів. Також вибрив підхід аналізу підходу для цієї задачі,\n",
        "бо інша людина займається аналізом та підготовкою даних, і я маю час для певної підготовки коду та\n",
        "й для вибору підходу для побудови моделі. Використоавши цей час, та зробивши нижче наведені висновки,\n",
        "я тому і вибрав для такої задачі і такого Датасету саме логістичну регресію. Та також враховуючи, обмеження в часі, після аналізу підходів, я зупиняюся на цьому.\n",
        "\n",
        "Тобто модель повинна класифікувати кожного клієнта в одну з двох груп: клієнт, який припинить користуватися послугами компанії\n",
        "(покине нас - Клас 1 (позитивний клас)), або клієнт, який продовжить користуватися послугами компанії (залишиться з нами - Клас 0 (негативний клас)).\n",
        "\n",
        "Детальніше про розробку моделі:\n",
        "- модель розробляємо, щоб виявити ймовірність відтоку клієнтів на основі зібраних реальних даних\n",
        "- реалізовувати логіку будемо через підхід логістичної регресії\n",
        "- аналізуватися будуть всі дані про клієнтів, які наведені в Датасеті\n",
        "- використовуватимемо мітку - значення (1 або 0), тобто факт відтоку (клієнт може нас покинути) або його відсутність (клієнт певно лишиться з нами).\n",
        "\n",
        "Я вибрав саме підхід логістичної регресії, бо цей підхід надає ймовірність приналежності до класу для кожної характеристики клієнта\n",
        "(надає розуміння впливу будь-якої ознаки на ймовірність відтоку), його досить легко прописати і коригувати (тобто реалізувати),\n",
        "добре працює з різними наборами даних та може надати хорошу точність якщо дані правильно підготовлені (зазвичай надає високу точність прогнозування на багатьох наборах даних).\n",
        "\n",
        "Тобто логістична регресія полягає в її здатності надавати легко інтерпретовані ймовірнісні оцінки,\n",
        "що робить її особливо корисною для задач бінарної класифікації,\n",
        "таких як прогнозування відтоку клієнтів.\n",
        "Це дозволяє аналітикам не тільки передбачати результат, але й розуміти ступінь впевненості у цих прогнозах, що є критично важливим для прийняття бізнес-рішень.\n",
        "\n",
        "Під час навчання моделі буду використовувати крос-валідацію для налаштування параметрів моделі, для наступного:\n",
        "- уникнення перенавчання (це коли модель занадто добре підлаштовується під навчальні дані і не може обробляти нові з очікуваними результатами)\n",
        "- оптимізації гіперпараметрів (Для моделі може існувати багато гіперпараметрів, які потрібно налаштовувати.\n",
        "  Крос-валідація дозволяє ефективно тестувати різні комбінації гіперпараметрів на декількох підмножинах даних і вибирати оптимальні значення, які забезпечують найкращу продуктивність моделі)\n",
        "- забезпечення більної стабільності моделі (Крос-валідація використовує кілька підмножин даних для навчання і тестування.\n",
        "  Це дозволяє моделі краще узагальнюватися і бути менш залежною від конкретного поділу даних.)\n",
        "- підтримання балансу між складністю мобелі та її здатністю оптимально опрацьовувати нові набори даних\n",
        "- оцінювання моделі через призму різних метрик\n",
        "Тому крос-валідацію краще застосовувати при налаштуванні параметрів моделі для забезпечення більшої точності,\n",
        "стабільності та узагальненості (так би мовити \"здатності очікувано давати прогноз на реальних, не навчальних даних\") моделі,\n",
        "яка може ефективно працювати на нових даних і допомагає уникнути перенавчання.\n",
        "\n",
        "Наступні підходи не викорисуються, бо\n",
        "- 1 - Лінійна регресія - краще підходить для задач передбачення кількісних значень, тоді як завдання прогнозування відтоку клієнтів є бінарною класифікацією\n",
        "може передбачити значення, що лежать поза межами [0, 1], що не має сенсу для ймовірностей\n",
        "- 2 - Метод опорних векторів (SVM) - є обчислювально витратними, особливо для великих наборів даних\n",
        "та не надає безпосередньої інтерпретації у вигляді ймовірностей, що може бути корисним для бізнес-аналітики\n",
        "- 3 - Дерева рішень - схильні до перенавчання; Хоча дерева рішень легко інтерпретуються, вони можуть бути нестабільними,\n",
        "оскільки навіть невеликі зміни у даних можуть призвести до значних змін у структурі дерева\n",
        "- 4 - Кластеризація (K-means) - підходить для задач сегментації або кластеризації, де потрібно розбити дані на групи, але не для бінарної класифікації\n",
        "- 5 - Нейронні мережі - Навчання нейронних мереж вимагає значних обчислювальних ресурсів і часу, що може бути надмірним для задачі з невеликою кількістю ознак\n",
        "\n",
        "При оціюванні точноті моделі планую використати метрики точності, повноти, точності передбачення та F1-міри."
      ],
      "metadata": {
        "id": "CokP4I6pmOw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# імпортуємо необхідні бібліотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
      ],
      "metadata": {
        "id": "ue8SpZCrgpu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# завантаження даних - потім переробиться на підготовлені дані\n",
        "data = pd.read_csv('/mnt/data/internet_service_churn.csv')"
      ],
      "metadata": {
        "id": "5UiY7HH_guBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Розділяємо дані на ознаки (X) та мітки (y)\n",
        "# видаляємо колонку 'сhurn' з набору даних data, залишаючи всі інші колонки як ознаки. axis=1 означає, що ми видаляємо колонку, а не рядок\n",
        "x = data.drop('churn', axis=1)  # будуть ознаки - є даними, на основі яких ми будемо робимо прогнози\n",
        "\n",
        "# вибираємо колонку 'сhurn' як цільову змінну, яку модель буде прогнозувати\n",
        "y = data['churn'] # це мітка - є тим, що ми намагаємося передбачити"
      ],
      "metadata": {
        "id": "iEOyoOcugzjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Використовуємо train_test_split для розподілу даних на навчальну та тестову вибірки\n",
        "\n",
        "- test_size: відсоток даних, що будуть використані для тестування\n",
        "в нашому вмпадку 20 % (зазвичай буває 20-30 %)\n",
        "\n",
        "- random_state є параметром, який використовується для фіксації початкового значення генератора випадкових чисел\n",
        "фіксоване значення random_state дозволяє будь-кому отримувати однакові результати при кожному запуску коду"
      ],
      "metadata": {
        "id": "oRnDjIClg2H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# розподіл даних на навчальну та тестову вибірки\n",
        "x_trained, x_tested, y_trained, y_tested = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# ініціалізація моделі логістичної регресії\n",
        "# max_iter=1000 - аргумент, який задає максимальну кількість ітерацій алгоритму оптимізації\n",
        "# параметр max_iter гарантує, що алгоритм не зациклиться і завершиться після певної кількості кроків навіть, якщо не досягне оптимального розв'язку\n",
        "our_model = LogisticRegression(max_iter=1000)"
      ],
      "metadata": {
        "id": "10SsyMp3hGCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Параметри для крос-валідації\n",
        "1 - 'C' - це параметр регуляризації у логістичній регресії. Він контролює ступінь регуляризації (тобто, обмеження складності моделі)\n",
        "- дозволяє знайти оптимальний баланс між підгонкою та узагальненням\n",
        "- мале значення C означає сильну регуляризацію, тоді як велике значення C означає слабку регуляризацію\n",
        "- ми обрали значення C у діапазоні від 0.01 до 100, щоб охопити широкий спектр можливих значень регуляризації\n",
        "- цей діапазон дозволяє моделі знайти оптимальний баланс між перенавчанням і недонавчання\n",
        "- надто малі значення (наприклад, 0.0001) можуть призвести до надмірної регуляризації, що може зменшити продуктивність моделі\n",
        "- надто великі значення (наприклад, 1000) можуть призвести до слабкої регуляризації, що може призвести до перенавчання\n",
        "\n",
        "Значення 0.01, 0.1: більш сильна регуляризація (запобігає перенавчанню, але може недонавчити) - роблять регуляризацію сильнішою, зменшуючи ризик перенавчання, але можуть призвести до недонавчання\n",
        "\n",
        "Значення 1: стандартне значення, часто використовується як відправна точка\n",
        "\n",
        "Значення 10, 100: менш сильна регуляризація (може краще підійти для складніших моделей) - послаблюють регуляризацію, що може допомогти моделі краще адаптуватися до тренувальних даних, але підвищує ризик перенавчання\n",
        "\n",
        "2 - 'solver' - це алгоритм, який використовується для оптимізації функції втрат у логістичній регресії\n",
        "- обидва вибрані алгоритми є загальновизнаними та ефективними для широкого спектру задач, вони забезпечують баланс між ефективністю навчання та загальною продуктивністю моделі\n",
        "- інші алгоритми solver , наприклад newton-cg', 'sag', 'saga' можуть бути ефективними, але часто складніші для налаштування та можуть вимагати більше часу для навчання, також sag' та 'saga' часто використовуються для великих наборів даних, але можуть не забезпечувати кращу продуктивність для середніх або малих наборів даних\n",
        "- тому нші варіанти можуть бути менш оптимальними через збільшення складності, часу на навчання або неадекватну регуляризацію\n",
        "\n",
        "Значення lbfgs': Limited-memory Broyden–Fletcher–Goldfarb–Shanno (ефективний для невеликих і середніх за розміром наборів даних),  добре працює з великими наборами даних та високовимірними проблемами\n",
        "\n",
        "Значення liblinear': лінійний солвер, підходить для великих наборів даних та є ефективним для малих та середніх розмірів наборів даних та забезпечує швидке навчання\n",
        "\n",
        "Всі ці параметри важливі для забезпечення оптимальної продуктивності моделі, запобігання перенавчанню та адаптації до структури даних. Вибір гіперпараметрів є важливим кроком у побудові та налаштуванні моделі машинного навчання"
      ],
      "metadata": {
        "id": "K6tKHwsJhJsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# параметри для крос-валідації\n",
        "# визначаємо набір гіперпараметрів, які будуть використовуватися для налаштування моделі\n",
        "parameters = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['lbfgs', 'liblinear']\n",
        "}"
      ],
      "metadata": {
        "id": "geJusLhaiOIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Чому інші параметри логістичної регресії не використовувались?\n",
        "\n",
        "1 - penalty - цей параметр визначає тип регуляризації, який буде застосований до моделі; можливі значення: l1, l2, elasticnet, none.Пприпускаємо, що 'l2' (норма Тихонова) є достатньо універсальною для нашої задачі. Інші варіанти, такі як 'l1' (LASSO) або 'elasticnet', можуть бути корисні в ситуаціях, коли є підозра на високу корельованість ознак або коли потрібне стиснення моделі. Проте, ці варіанти можуть вимагати додаткової налаштування і обчислювальних ресурсів\n",
        "\n",
        "2 - max_iter - це максимальна кількість ітерацій для оптимізатора, за замовчуванням значення встановлено на 100, ми вже встановили max_iter=1000 при ініціалізації моделі, що забезпечує достатню кількість ітерацій для збіжності алгоритму\n",
        "\n",
        "3 - tol - це допуск для критерію зупинки, визначає, наскільки зміна у функції втрат повинна бути малою для зупинки навчання, значення за замовчуванням зазвичай є достатнім для більшості задач, зміна цього параметру може бути необхідною лише в ситуаціях, коли спостерігаються проблеми зі збіжністю\n",
        "\n",
        "4 - class_weight - вага класів у задачі класифікації, можна встановити 'balanced' для автоматичного зважування класів залежно від їх частоти, може бути корисним у випадках значного дисбалансу класів; припускаємо, що класовий дисбаланс не є критичним або що його було враховано під час підготовки даних\n",
        "\n",
        "5 - random_state - насіння для генератора випадкових чисел, яке забезпечує відтворюваність результатів, ми його зазначили як 123, коли розподіляли дані на навчальну та тестову вибірки для забезпечення відтворюваності\n",
        "\n",
        "Саме тому, ми обмежилися основними параметрами, оскільки вони є найважливішими\n",
        "і забезпечують базовий рівень налаштування, достатній для побудови ефективної моделі"
      ],
      "metadata": {
        "id": "t0XWzUiWiRQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Налаштовуємо пошук найкращих гіперпараметрів для моделі логістичної регресії за допомогою методу крос-валідації\n",
        "\n",
        "1 - 'GridSearchCV' - це клас у бібліотеці scikit-learn, який використовується для автоматичного підбору найкращих гіперпараметрів моделі, він перебирає всі можливі комбінації гіперпараметрів і оцінює продуктивність кожної комбінації за допомогою крос-валідації; вибирає ту комбінацію гіперпараметрів, яка показала найкращу середню точність на крос-валідації\n",
        "\n",
        "2 - 'estimator' вказує на модель, для якої ми хочемо підібрати гіперпараметри\n",
        "\n",
        "3 - 'param_grid' задає сітку гіперпараметрів, які потрібно перебирати, це словник, де ключі - це імена параметрів, а значення - списки можливих значень цих параметрів\n",
        "\n",
        "4 - 'cv' означає кількість фолдів у крос-валідації, ми використовуємо 5-кратну крос-валідацію, тобто дані розбиваються на 5 частин, і кожна частина по черзі використовується як тестовий набір, тоді як інші частини використовуються як тренувальні набори\n",
        "\n",
        "Чому використовуємо 'cv=5'?\n",
        "- 5-кратна крос-валідація забезпечує гарний компроміс між точністю оцінки моделі та обчислювальними ресурсами\n",
        "- збільшення кількості фолдів до 10 чи 20 може покращити точність оцінки, але вимагає значно більше обчислень\n",
        "- зазвичай, 5-кратна крос-валідація дає достатньо стабільні оцінки метрик продуктивності\n",
        "- менше значення (наприклад, 3) може призвести до менш стабільних оцінок,\n",
        "- а більше значення (наприклад, 10) дає лише незначне покращення стабільності, але з більшими витратами ресурсів\n",
        "\n",
        "5 - 'scoring' задає метрику, яку використовують для оцінки продуктивності моделі, таким чином ми забезпечуємо автоматичний пошук оптимальних гіперпараметрів, запобігаємо перенавчанню та також використання крос-валідації з різними фолдами; він надає більш об'єктивну оцінку продуктивності моделі порівняно з простою тренувальною та тестовою вибіркою\n",
        "При роботі з підготовленими даними можливо треба буде використати іншу метрику, якщо дані будуть незбалансовані, наприклад F1-мірка (F1 Score) - може бути гарним компромісом, враховуючи як точність, так і повноту, особливо в умовах незбалансованих даних"
      ],
      "metadata": {
        "id": "90nvXnRTi22h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# крос-валідація\n",
        "# використовуємо GridSearchCV для пошуку кращих параметрів\n",
        "search_parameters = GridSearchCV(estimator=our_model, param_grid=parameters, cv=5, scoring='accuracy')\n",
        "\n",
        "# навчаємо модель з кращими параметрами на повному навчальному наборі даних\n",
        "# метод fit тренує об'єкт GridSearchCV на заданих навчальних даних\n",
        "search_parameters.fit(x_trained, y_trained)"
      ],
      "metadata": {
        "id": "KdMb742TjoPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Отримуємо найкращі гіперпараметри\n",
        "- '.best_params_' - це атрибут об'єкта GridSearchCV, який зберігає найкращі гіперпараметри, знайдені під час пошуку та містить словник з найкращими значеннями гіперпараметрів\n",
        "\n",
        "це нам дозволяє:\n",
        "- оптимізувати продуктивність - використання найкращих гіперпараметрів дозволяє досягти максимальної точності моделі на основі навчальних даних\n",
        "- автоматизувати процес - GridSearchCV автоматично перебирає всі можливі комбінації гіперпараметрів, що значно спрощує роботу\n",
        "- отримати надійність оцінки - використання крос-валідації забезпечує надійність результатів, оскільки модель оцінюється на різних підмножинах даних"
      ],
      "metadata": {
        "id": "4giSUiGEjsc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# отримуємо найкращі гіперпараметри, знайдені у процесі пошуку за допомогою GridSearchCV\n",
        "best_parameters = search_parameters.best_params_\n",
        "\n",
        "# друкуємо найкращі параметри\n",
        "print(f'Best parameters: {best_parameters}')\n",
        "\n",
        "# навчання моделі з кращими параметрами\n",
        "# атрибут 'best_estimator_' зберігає модель з найкращою комбінацією гіперпараметрів\n",
        "# ця модель тепер готова до використання для прогнозування на нових даних\n",
        "best_model = search_parameters.best_estimator_"
      ],
      "metadata": {
        "id": "0jKgs--oj-As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Виконуємо навчання моделі best_model на навчальних даних x_trained і мітках y_trained\n",
        "- 'x_trained' це навчальні дані, які включають ознаки (фічі), використані для навчання моделі, в нашому випадку це всі колонки датасету, крім колонки міток сhurn\n",
        "\n",
        "- 'y_trained' це мітки, що вказують на цільові значення (в даному випадку Churn), які модель має навчитися передбачати. Мітки вказують, чи залишився клієнт (0) або припинив користування послугами (1)\n",
        "\n",
        "Після виконання fit модель налаштована на найкращі можливі параметри, що дозволяє їй робити точні прогнози на нових, невідомих даних."
      ],
      "metadata": {
        "id": "d0HshFGBkCsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# виконуємо навчання моделі на навчальних даних x_trained і мітках y_trained\n",
        "best_model.fit(x_trained, y_trained)"
      ],
      "metadata": {
        "id": "NlSaWOaQkPjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Робимо передбачення на тестовій вибірці,\n",
        "тобто виконуємо передбачення результатів для тестових даних x_tested за допомогою навченого найкращого моделі best_model\n",
        "\n",
        "- 'y_predicted' це змінна, яка буде містити передбачені мітки для тестових даних, ці передбачення є результатом роботи моделі після її навчання\n",
        "\n",
        "- 'predict' це метод, що використовується для передбачення міток для нових даних на основі навчених параметрів моделі\n",
        "\n",
        "- 'x_tested' це тестові дані, на яких ми хочемо перевірити, наскільки добре модель може робити передбачення, ці дані не використовувалися під час навчання моделі і служать для оцінки її продуктивності; передбачення буде або 0 (клієнт не відмовився від послуг), або 1 (клієнт відмовився від послуг)\n",
        "\n",
        "Таким чином це допомагає нам зрозуміти, наскільки модель буде ефективною в реальних умовах, де ми будемо використовувати її для передбачення результатів для нових клієнтів, які не були частиною початкових даних"
      ],
      "metadata": {
        "id": "VW90n7wjkS4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# робимо передбачення на тестовій вибірці\n",
        "y_predicted = best_model.predict(x_tested)"
      ],
      "metadata": {
        "id": "YznicRT2koxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# оцінка моделі\n",
        "# точність визначає частку правильних передбачень серед усіх передбачень\n",
        "# надає загальну оцінку, як добре модель класифікує дані\n",
        "# ця функція з бібліотеки scikit-learn обчислює точність моделі, порівнюючи фактичні мітки y_tested з передбаченими мітками y_predicted.\n",
        "accuracy = accuracy_score(y_tested, y_predicted)\n",
        "\n",
        "# повнота визначає частку правильно передбачених позитивних прикладів серед усіх фактичних позитивних прикладів\n",
        "recall = recall_score(y_tested, y_predicted)\n",
        "\n",
        "# точність визначає частку правильно передбачених позитивних прикладів серед усіх передбачених позитивних прикладів\n",
        "# важлива, коли вартість помилково позитивних прикладів висока\n",
        "precision = precision_score(y_tested, y_predicted)\n",
        "\n",
        "# є гармонійним середнім між точністю і повнотою\n",
        "# є важливою, коли потрібно збалансувати точність і повноту\n",
        "f1 = f1_score(y_tested, y_predicted)\n",
        "\n",
        "# виводимо результати оцінки\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "id": "qKHUm3c4lpyc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}